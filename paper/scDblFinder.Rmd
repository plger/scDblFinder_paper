---
title: "Doublet identification in single-cell sequencing data using scDblFinder"
author: 
  - name: Pierre-Luc Germain
    affiliation: DMLS Lab of Statistical Bioinformatics, UZH; D-HEST Institute for Neuroscience, ETH; Swiss Institute of Bioinformatics
  - name: Aaron Lun
    affiliation: Genentech Inc., South San Francisco, CA USA
  - name: Carlos Garcia Meixide
  - name: Will Macnair
    affiliation: F. Hoffmann-LaRoche Ltd, Roche Innovation Center Basel
  - name: Mark D. Robinson
    affiliation: DMLS Lab of Statistical Bioinformatics, UZH; Swiss Institute of Bioinformatics
output:
  bookdown::html_document2:
    code_folding: hide
    number_sections: false
bibliography: "`r rbbt::bbt_write_bib('biblio.json', overwrite = TRUE)`"
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE)
```


```{r, include=FALSE}
suppressPackageStartupMessages({
  library(SingleCellExperiment)
  library(ComplexHeatmap)
  library(scDblFinder)
  library(ggplot2)
  library(cowplot)
  library(MASS)
  library(nnls)
  library(edgeR)
  library(ggrepel)
  library(BiocStyle)
  library(patchwork)
})
theme_set(theme_minimal())
source("../misc.R")

suppFigN <- 0L
suppFig <- function(increment=NULL, nb=NULL){
  if(is.null(increment)) increment <- is.null(nb)
  if(!isFALSE(increment)){
    assign("suppFigN", suppFigN+as.integer(increment), envir=.GlobalEnv)
    if(increment>1 && is.null(nb)) nb <- paste0(suppFigN-increment+1,"-",suppFigN)
  }
  if(is.null(nb)) nb <- suppFigN
  if(nb<0) nb <- suppFigN+nb
  paste0("Extended Data - Figure ", nb)
}
```

# Abstract

Doublets are prevalent in single-cell sequencing data and can lead to artifactual findings.
A number of strategies have therefore been proposed to detect them.
Building on the strengths of existing approaches, we developed `scDblFinder`, 
a fast, flexible and accurate Bioconductor-based doublet detection method.
Here we present the method, justify its design choices, demonstrate its performance on both single-cell RNA and 
accessibility sequencing data, and provide some observations on doublet formation, detection, and enrichment analysis.
Even in complex datsets, `scDblFinder` can accurately identify most heterotypic doublets, 
and was already found by an independent benchmark to outcompete alternatives.

Doublets are prevalent in single-cell sequencing data and can lead to artifactual findings.
A number of strategies have therefore been proposed to detect them.
Building on the strengths of existing approaches, we developed `scDblFinder`,
a fast, flexible and accurate Bioconductor-based doublet detection method.
Here we present the method, justify its design choices, demonstrate its performance on both single-cell RNA and accessibility (ATAC) sequencing data, and provide some observations on doublet formation, detection, and enrichment analysis.
Even in complex datasets, scDblFinder can accurately identify most heterotypic doublets,
and was already found by an independent benchmark to outcompete alternatives.


# Keywords

Single-cell sequencing, scRNA-seq, scATAC-seq, doublets, multiplets, filtering

# Introduction

High-throughput single-cell sequencing, in particular single-cell/nucleus RNA-sequencing (scRNAseq), has provided an unprecedented resolution on biological phenomena.
A particularly popular approach uses oil droplets or wells to isolate single cells along with barcoded beads.
Depending on the cell density loaded, a proportion of reaction volumes (i.e. droplets or wells) will capture more than one cell, forming 'doublets' (or 'multiplets'), i.e. two or more cells captured by a single reaction volume and thus sequenced as a single-cell artifact.
The proportion of doublets has been shown to be proportional to the number of cells captured [@bloomEstimatingFrequencyMultiplets2018; @kangMultiplexedDropletSinglecell2018].
It is therefore at present common in single-cell experiments to have 10-20% doublets, making accurate doublet detection critical.

To avoid confusion, we will denote as 'droplet' the reads that are assigned to one barcode (either doublet or singlet), and reserve the term 'cells' to talk about original (singlet) cells. 'Homotypic' doublets, which are formed by cells of the same type (i.e. similar transcriptional state), are very difficult to identify on the basis of their transcriptome alone [@mcginnisDoubletFinderDoubletDetection2019]. They are also, however, relatively innocuous for most purposes, as they appear highly similar to singlets.
'Heterotypic' doublets (formed by cells of distinct transcriptional states), instead, can appear as an artifactual novel cell type and disrupt downstream analyses [@germainPipeCompGeneral2020b].

Experimental methods have been devised for detecting doublets in multiplexed samples, using barcodes [@stoeckiusCellHashingBarcoded2018] or genotypes (e.g. single-nucleotide polymorphisms) to identify droplets containing material from more than one sample [@kangMultiplexedDropletSinglecell2018].
While evidently useful, these often incur additional costs or limitations.
Furthermore, they identify only a fraction of the doublets, and fail to detect doublets formed by cells from the same sample, including heterotypic doublets.
The proportion of doublets missed will decrease with the degree of multiplexing, but even mixing 10 samples would result in 10\% of the doublets missed; moreover, these approaches are not always applicable.

A number of computational approaches have therefore been developed to identify doublets on the basis of their transcriptional profile [@mcginnisDoubletFinderDoubletDetection2019; @depasqualeDoubletDeconDeconvoluting2019; @wolockScrubletComputationalIdentification2019; @baisScdsComputationalAnnotation2020; @bernsteinSoloDoublet2020].
Most of these approaches rely on the generation of artificial doublets by summing or averaging reads from real droplets, and score the similarity between them and the real droplets.
For example, <a href="https://github.com/chris-mcginnis-ucsf/DoubletFinder">`DoubletFinder`</a> generates a _k_-nearest neighbor (kNN) graph on the union of real droplets and artificial doublets, and estimates the density of artificial doublets in the neighborhood of each droplet [@mcginnisDoubletFinderDoubletDetection2019].
In a similar fashion, one of the methods proposed by @baisScdsComputationalAnnotation2020, <a href="https://bioconductor.org/packages/release/bioc/html/scds.html">`bcds`</a>, generates artificial doublets and trains a classifier to distinguish them from real droplets.
Droplets that are classified with artificial doublets are then called as doublets.
Finally, another strategy proposed by @baisScdsComputationalAnnotation2020 is a coexpression score, <a href="https://bioconductor.org/packages/release/bioc/html/scds.html">`cxds`</a>, which flags droplets that co-express a number of genes that otherwise tend to be mutually exclusive.

@xiBenchmarkingComputationalDoubletDetection2021 recently reported a benchmark of computational doublet detection methods, using both simulations and real datasets with annotated true doublets.
Interestingly, despite several new publications, the initial benchmark identified the oldest method, <a href="https://github.com/chris-mcginnis-ucsf/DoubletFinder">`DoubletFinder`</a> [@mcginnisDoubletFinderDoubletDetection2019].
However, another important observation from the benchmark was that no single method was systematically the best across all datasets, highlighting the necessity to test and benchmark methods across a variety of datasets, and suggesting that some strategies might have advantages and disadvantages across situations.

Here, we present the `r Biocpkg("scDblFinder")` package, building on the extensive single-cell `Bioconductor` methods and infrastructures [@amezquitaOrchestratingSinglecellAnalysis2019] and implementing a number of doublet detection approaches.
In particular, the `scDblFinder` method integrates insights from previous approaches and novel improvements to generate fast, flexible and robust doublet prediction. `scDblFinder` was independently tested by Xi and Li in the protocol extension to their initial benchmark and was found to have the best overall performance [@xiProtocolExecutingBenchmarking2021].

# Results

## Simulation of artificial doublets

As most approaches rely on some comparison of real droplets to artificial doublets, it is crucial to appropriately simulate doublets. To this end, we first characterized real doublets using a dataset of genetically distinct cell lines [@tianScRNAseqMixologyBetter2018]. Because each cell line represents a distinct and more or less homogeneous transcriptional state, it is possible, using genotypes, to identify the `cell types' composing each doublet (Figure \@ref(fig:realDbls)).
Although often larger, the median library sizes of doublets were systematically smaller than the sum of the median library sizes of composing cell types (Figure \@ref(fig:realDbls)A).

```{r realDbls, fig.width=9, fig.height=8, fig.cap="**Characterization of real doublets in a mixture of three human lung adenocarcinoma cell lines. A:** Observed median (and +/- one median absolute deviation in) library sizes per cell type against additive expectation for single cell and doublet types in a real dataset. The dashed line indicates the diagonal. **B:** Relative contribution of composing cell types in real doublets (each point represents a doublet) plotted against the expected relative contributions (based on the ratio between the median library sizes of the composing cell types). Values indicate the relative contribution of one of the two cell types to the doublet's transcriptome. The dashed line indicates the diagonal, and the thick line indicates the weighted mean per doublet type. The annotation of cell types and their combinations comes from the original Demuxlet analysis by Tian et al., excluding ambiguous calls."}
# real doublet sizes compared to expected sizes
sce <- readRDS("../other_datasets/mixology10x5cl.SCE.rds")
md <- colData(sce)
md$class <- as.character(md$phenoid)
md$isDoublet <- md$demuxlet_cls=="DBL"
w <- which(md$isDoublet)
md$class[w] <- as.character(md$demuxlet.dbl.type[w])

ag <- aggregate(md$total_counts, by=list(class=md$class), FUN=median)
row.names(ag) <- ag$class
ag$isDoublet <- grepl("+",ag$class,fixed=TRUE)
ag$mad <- aggregate(md$total_counts, by=list(class=md$class), FUN=mad)[,2]
ag$sum <- sapply(strsplit(ag$class,"+",fixed=TRUE), FUN=function(x) sum(ag[x,"x"]))

p1 <- ggplot(ag, aes(sum, x, colour=isDoublet, label=class)) + 
  geom_abline(linetype="dashed", colour="grey") + geom_point(size=2) +
  geom_segment(aes(x=sum,xend=sum,y=x-mad,yend=x+mad)) + 
  geom_text_repel(colour="black", min.segment.length=0) + 
  labs(x="Sum of median library sizes", y="Observed median library size")

# relative contributions of composing cells
dbls <- split(w, md$class[w])
dbls <- dbls[grep("\\+",names(dbls))]
cs <- sapply(split(seq_len(nrow(md))[-w],md$phenoid[-w]), FUN=function(i) rowSums(assay(sce)[,i,drop=FALSE]))
cs <- as.matrix(edgeR::cpm(calcNormFactors(DGEList(cs))))
dbls <- dplyr::bind_rows(lapply( setNames(names(dbls),names(dbls)), FUN=function(comb){
  orig <- strsplit(comb,"+",fixed=TRUE)[[1]]
  data.frame(
    lsize=colSums(assay(sce)[,dbls[[comb]],drop=FALSE]),
    expectedProp=ag[orig[1],"x"]/(ag[orig[1],"x"]+ag[orig[2],"x"]),
    prop.type1=apply(as.matrix(assay(sce)[,dbls[[comb]],drop=FALSE]), 2, FUN=function(x){
      tryCatch({
        mod <- nnls::nnls(cs[,orig],x)
        coef(mod)[1]/sum(coef(mod))
      }, error=function(e) NA)
    }))
}), .id="doublet.type")
saveRDS(dbls, file="data/doublet_contributions.rds")
ag2 <- t(sapply(split(seq_len(nrow(dbls)), dbls$doublet.type), FUN=function(w){
  c(ratio=weighted.mean(dbls$prop.type1[w], dbls$lsize[w], na.rm=TRUE),
    expected.ratio=dbls$expectedProp[w[1]])
}))
ag2 <- data.frame(doublet.type=row.names(ag2), ag2)
p2 <- ggplot(dbls, aes(expectedProp, prop.type1)) + geom_abline(linetype="dashed", col="grey") +
  geom_point(aes(size=lsize, colour=doublet.type)) + scale_color_discrete(guide=FALSE) +
  geom_line(data=ag2, aes(expected.ratio, ratio), size=1.5, colour="grey") + 
  geom_text_repel(data=ag2, aes(expected.ratio, ratio, label=doublet.type), direction="y", nudge_y=c(0.1,-0.1)) +
  labs(x="Expected proportion (based on median cell type library size)",
       y="Estimated proportion", size="Library size")

# pdf("Figure1.pdf", width=9, height=8)
# plot_grid(p1, p2, labels="AUTO", nrow=2, scale=0.95)
# dev.off()
plot_grid(p1, p2, labels="AUTO", nrow=2, scale=0.95)
```

We next investigated the relative contributions of the composing cell types using non-negative least squares regression, expecting the larger cell types to contribute more to the doublet's transcriptome. Although differences in median library size across cell types were small (less than two-fold) compared to other datasets, we observed an association of the relative contributions with the relative sizes of the composing cell types (Figure \@ref(fig:realDbls)B, p~`r format(coef(summary(lm(ratio~0+expected.ratio, data=ag2)))[1,4], digits=2)`).
However, this effect was very weak -- considerably smaller than the variation within doublet type.
This suggests that there are 
i) large variations in real cell size within a given cell type, and/or
ii) large variations in the mRNA sampling efficiency that are independent for the two composing cells.
Either way, for many doublets, the two composing cell types contribute very unequally. This also explains why, while doublets sometimes form their own clusters, they often appear at the periphery of the singlet cluster they most resemble.

In light of these ambiguities, we opted for a mixed strategy to simulate artificial doublets, generating them in a combination of three different ways (see methods):
a proportion is generated by summing the libraries of individual cells,
another by performing a poisson resampling of the obtained counts,
and a third by re-weighting the contributions of cells depending on the relative median sizes of the composing cell types (in case the observed library size is a poor indicator of RNA content).
This strategy did not lead to a clear overall improvement across the datasets (`r suppFig()`A) over the simple sum (both of which were clearly superior to averaging), suggesting that most of the difference is anyway within the wide variability in library sizes, and/or that the normalization and dimensionality reduction steps are sufficient to remove remaining differences between real and artificial doublets. Another possible interpretation is that doublets can be approximated as the sum of the counts of the composing cells, but that doublets composed of larger cells are less likely to form. Either way, since it was not deleterious and might prove more robust to variations in protocols, we nevertheless maintained the mixed strategy, generating the majority of doublets (75\%) using the simple sum, and the rest using the mixed strategy.


## scDblFinder consistently outperforms alternative methods

```{r strategy, out.width="70%", fig.align="center", fig.cap="Overview of the scDblFinder method."}
knitr::include_graphics("strategy2.svg")
```

Figure \@ref(fig:strategy) gives an overview of the `scDblFinder` method (see [Methods] for details).
Briefly, after some initial processing, artificial doublets (either random or between-cluster, depending on the settings) are generated, then a nearest neighbor (kNN) network is generated.
Rather than selecting a single neighborhood size, as most kNN-based methods do, `scDblFinder` gather statistics at various neighborhood sizes, thereby enabling the downstream classifier to select the most informative size(s), which might also differ across the expression space.
Various characteristics from each cell/doublet and its neighborhood (such as the density of artificial doublets in the neighborhood) are then gathered to build a cell-level predictors matrix.
On the basis of these predictors, a classifier is trained to distinguish artificial doublets from droplets.
A key problem with classifier-based approaches is that some of the droplets are mislabeled, in the sense that they are in fact doublets labeled as singlets.
These can mislead the classifier. For this reason, classification is performed in an iterative fashion:
at each round, the droplets confidently identified as doublets are removed from the training data for the next round. Similarly, when using randomly-generated (as opposed to between-cluster) artificial doublets, those deemed unidentifiable are removed for the training. 
We found that 2-3 iterations provided the best performance (`r suppFig(increment=FALSE)`B).

A previous version of `scDblFinder` was already evaluated in an independent benchmark [see @xiProtocolExecutingBenchmarking2021, the protocol and addendum to the original study, @xiBenchmarkingComputationalDoubletDetection2021], where it was found superior to existing alternatives across a variety of metrics.
Here we reproduced this benchmark using the most recent versions of the packages, and including variant methods from the `scDblFinder` package (among which the updated version of `r Biocpkg("scran")`'s original method, and now available in the `scDblFinder` package as `computeDoubletDensity`). In addition, we included the new method `Chord` [@xiongChordIdentifyingDoublets2021], which also combines different strategies.

Figure \@ref(fig:benchmark1) compares the performance of `scDblFinder` to alternatives across real benchmark datasets, as measured by the area under the precision-recall (PR) curve (AUPRC) in classifying annotated doublets. Where there is sufficient data for training, `scDblFinder` is consistently the top performer, except for very simple datasets where most methods perform very well. Calculating the mean AUPRC across the datasets, the top two methods are the two `scDblFinder` variants (`r suppFig(increment=FALSE)`C). In addition, `scDblFinder` runs at a fraction of the time required by the next best methods (Figure \@ref(fig:benchmark1), left).

```{r benchmark1, fig.width=9.5, fig.height=5.8, fig.cap="**Benchmark.** Accuracy (area under the precision and recall curve) of doublet identification using alternative methods across 16 benchmark datasets. The colour of the dots indicates the relative ranking for the dataset, while the size and numbers indicate the actual area under the (PR) curve. For each dataset, the top method is circled in black. Methods with names in black are provided in the scDblFinder package. Running times are indicated on the left. On top the number of cells in each dataset is shown, and colored by the proportion of variance explained by the first two components (relative to that explained by the first 100), as a rough guide to dataset simplicity."}
e <- readRDS("../benchmark/benchmark.results.rds")

res <- readRDS("../analyses/optims_direct.rds")
res <- res[res$processing=="direct rawFeatures",]
res <- aggregate(res[,c("AUPRC","AUROC","elapsed")], by=res[,"dataset",drop=FALSE], FUN=mean)
res$method <- "directDblClassification"
e <- rbind(e, res[,colnames(e)])

datmax <- sort(apply(reshape2::dcast(e, method~dataset, value.var="AUPRC")[,-1],
                     2,na.rm=TRUE,FUN=max))
metmax <- sort(apply(reshape2::dcast(e, dataset~method, value.var="AUPRC")[,-1],
                     2,na.rm=TRUE,FUN=median))
e$dataset <- factor(e$dataset, levels=names(datmax))
e$method <- factor(e$method, levels=names(metmax))
levels(e$method) <- gsub("bcds","scds::bcds",levels(e$method))
levels(e$method) <- gsub("cxds","scds::cxds",levels(e$method))
levels(e$method) <- gsub("hybrid","scds::hybrid",levels(e$method))

getranks <- function(x){
  y <- rank(x)
  y[is.na(x)] <- NA
  y
}

tr <- reshape2::dcast(e, method~dataset, value.var="AUPRC")
row.names(tr) <- tr[,1]; tr <- tr[,-1]
tr2 <- apply(tr,2,FUN=getranks)
e$AUPRC.rank <- apply(e[,1:2], 1, FUN=function(x) tr2[as.character(x[2]),as.character(x[1])])

e$point.colour <- viridisLite::viridis(100)[round(100*e$AUPRC)]
e$border.colour <- ifelse(e$AUPRC.rank>=(length(metmax)-0.5), "black", NA)
e$rounded <- round(e$AUPRC,2)
e$text.colour <- ifelse(e$rounded>=0.5,"black","white")
e$text.colour[e$AUPRC.rank==1] <- NA
e$text <- gsub("1\\.00","1.0",gsub("0\\.",".",sprintf("%.2f",e$AUPRC)))

nmeth <- length(unique(e$method))
scdbl.methods <- c("scDblFinder.clusters","scDblFinder.random","directDblClassification","computeDoubletDensity")

# # old version, rank as size
# p1 <- ggplot(e, aes(dataset, method)) + geom_point(aes(colour=point.colour, size=AUPRC.rank^2)) +
#   scale_size(range=c(4,11), breaks=c(1, (nmeth/2)^2, nmeth^2), labels=c("worst","","best")) +
#   geom_text(aes(label=rounded, colour=text.colour), size=3) + geom_point(size=11, shape=21, aes(colour=border.colour)) +
#   scale_color_identity() + labs(size="AUPRC rank") +
#   theme(axis.text.x=element_text(angle=45, hjust=1),
#         axis.text.y=element_text(hjust=0.5, size=10.5, face="bold", 
#                                  colour=ifelse(levels(e$method) %in% scdbl.methods,"black","grey30")), 
#         axis.title.y=element_blank(), panel.grid=element_blank())

p1 <- ggplot(e, aes(dataset, method)) + geom_point(aes(size=AUPRC, colour=AUPRC.rank)) + 
  scale_size(range=c(4,10)) + 
  scale_colour_viridis_c(breaks=c(1,10), labels=c("worst","best"), guide=guide_colorbar(barheight=unit(1, "cm"))) +
  geom_point(data=e[e$AUPRC.rank==length(unique(e$method)),], shape=21, colour="black", aes(size=AUPRC), stroke=1.1, show.legend=FALSE) + 
  geom_text(aes(label=text), size=3, colour=ifelse(e$AUPRC.rank>=5,"black","white")) + 
  labs(colour="AUPRC rank") +
  theme(axis.text.x=element_text(angle=45, hjust=1),
        axis.text.y=element_text(hjust=0.5, size=10.5, face="bold", 
                                 colour=ifelse(levels(e$method) %in% scdbl.methods,"black","grey30")), 
        axis.title.y=element_blank(), panel.grid=element_blank())

ag <- aggregate(e[,"elapsed",drop=FALSE], by=e[,"method",drop=FALSE], FUN=mean)
p2 <- ggplot(ag, aes(method, elapsed)) + geom_col(width=0.75, fill="#00204DFF") + 
  #scale_y_continuous(trans = ggforce::trans_reverser('sqrt'), breaks=c(10,60,300)) + 
  geom_text(aes(y=75,label=paste0(round(elapsed),"s"),colour=elapsed>200), hjust=1) + 
  scale_colour_manual(values=c("TRUE"="white", "FALSE"="black"), guide=FALSE) +
  scale_y_reverse() + coord_flip() + ylab("Mean running\ntime (s)") +
  theme(axis.text.y=element_blank(), axis.title.y=element_blank(), 
        axis.text.x=element_text(angle=90), panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank(), panel.grid.major.x=element_line(colour="lightgrey"))

m <- readRDS("data/dataset_desc.rds")
m$dataset <- factor(row.names(m), names(datmax))
p3 <- ggplot(m, aes(dataset, ncells, fill=ve)) + geom_col() + labs(x="",y="# cells") + 
  theme(axis.text.x=element_blank(), panel.grid.major.x=element_blank(),
        panel.grid.minor=element_blank()) +
  viridis::scale_fill_viridis(option="B", direction=-1, breaks=range(m$ve), labels=c("Low","High"),
                              name="Dataset\nsimplicity\n") +
  guides(fill=guide_colourbar(barheight=unit(1, "cm"))) +
  scale_y_sqrt(breaks=c(1000,5000,10000,20000))

#plot_grid(p2,p1,align="h", rel_widths=c(1,7))
p <- ((plot_spacer()/p2+plot_layout(heights=c(1,3))) | (p3/p1+plot_layout(heights=c(1,3.5)))) + plot_layout(widths=c(1,7))

# pdf("Figure3.pdf", width=9.5, height=5.8)
# p
# dev.off()
p
```

### kNN summarization improves upon direct classification

`scDblFinder` and the `bcds` method [@baisScdsComputationalAnnotation2020] are both based on a boosted classifier trained on artificial doublets, however `scDblFinder` performs considerably better. 
We hypothesized that this improvement would come from two main sources.
First, an improvement of `scDblFinder` is the iterative training, which prevents doublets among the real droplets (which are wrongly annotated as singlets) from misleading the classifier.
The observed impact of the iterative procedure (`r suppFig(increment=FALSE)`B) however suggests that it explains only part of the difference in performance.
Another important difference is that while `bcds` trains directly on the expression matrix, `scDblFinder` works chiefly on features of the kNN network.
Since artificial doublet creation can only approximate real doublets, we hypothesized that these differences are more likely to be apparent in the expression matrix than in the highly summarized set of features used by `scDblFinder`, and that this could lead to overfitting on the artificial problem.
Indeed, a risk of classifier-based approaches is that the problem on which the classifier is trained, namely distinguishing _artificial_ doublets from _real_ droplets, slightly differs from the real problem on which they are expected to function (distinguishing _real_ doublets from _real_ singlets).
To investigate this hypothesis of overfitting due, we implemented a version of `scDblFinder` without the dimensional reduction and kNN steps, and training the classifier directly on the expression of the selected genes.
This resulted in a reduction in area under the precision and recall curve (AUPRC) in real datasets (see [Direct classification]; see also Figure \@ref(fig:benchmark1) and `r suppFig()`).
We therefore conclude that, while dimensional reduction and kNN summarization arguably involve a loss of information, it nevertheless increases accuracy (in addition to considerably reducing computing time), presumably by preventing overfitting.

## scDblFinder identifies most heterotypic doublets

Several of the benchmark datasets have known true doublets flagged by mixing of single-nucleotide polymorphisms (SNPs) from multiple individuals [@kangMultiplexedDropletSinglecell2018].
In most of these cases, however, such annotation is an imperfect ground truth, for two reasons  (Figure \@ref(fig:adjustedPR)A, see also @mcginnisDoubletFinderDoubletDetection2019).
First, the doublets include also between-individual homotypic doublets, i.e. doublets formed by cells of the same type from different individuals. These are difficult to detect from gene expression, and are arguably of lower priority since they resemble real cells more closely.
More importantly, SNPs-based labels do not include heterotypic doublets that are the result of the combination of different cell types from the same individual.
It is therefore likely that the accuracy reported in the benchmark is below the actual one in detecting heterotypic doublets, and indeed datasets where there is a full correspondence between cell type and individual (such as the human-mouse mixtures hm-6k and hm-12k) typically have a much higher area under the Receiver-operator characteristic (ROC) and precision-recall (PR) curves (Figure \@ref(fig:benchmark1)).
Based on the frequency of the different individuals and cell types in a dataset, it is possible to infer the expected rate of between-individual homotypic doublets and within-individual heterotypic doublets.
This, in turn, enabled us to estimate the performance in identifying *heterotypic* doublets, as opposed to inter-individual but homotypic doublets.
Figure \@ref(fig:adjustedPR)B shows such an analysis for a complex dataset from @kangMultiplexedDropletSinglecell2018 .
The inflection point of the PR curve roughly coincides with the expected proportion of heterotypic doublets among those flagged as true doublets.

```{r adjustedPR, fig.width=9, fig.height=4, fig.cap="**Doublet types and real accuracy of heterotypic doublet identification. A:** Cartoon representing the different types of doublets. Within-individual heterotypic doublets will wrongly be labeled as false positives, and between-individual homotypic will be labeled as false negatives. **B:** Adjusted PR curve for an example sample (GSM2560248). The two shaded areas represent the expected proportion of within-individual heterotypic doublets (i.e. wrongly labeled as singlets in the annotation used as ground truth) and between-individual homotypic doublets, respectively. The red dotted line indicates the random expectation, and the black dot indicates the threshold set by scDblFinder.", warning=FALSE}
e <- readRDS("data/GSM2560248_noAmbiguous.processed.CD.rds")
# proportion homotypic doublets: these will be called as false negatives
prop.homotypic <- propHomotypic(e$scDblFinder.cluster)
# proportion within/intra-individual doublets: these will be called as false positives
prop.intraind <- propHomotypic(e$ind)
th <- min(e$scDblFinder.score[e$scDblFinder.class=="doublet"])
w <- which(e$scDblFinder.score>=th)
x <- as.integer(e$multiplets[w]=="doublet")[order(e$scDblFinder.score[w])]
d <- data.frame(x=sum(!x)/length(x),
                y=sum(x)/sum(e$multiplets=="doublet"))

p2 <- plotROCs(list(score=e$scDblFinder.score), e$multiplets=="doublet", fdr=TRUE, 
               prop.wrong.neg=prop.intraind, prop.wrong.pos=prop.homotypic, addNull=TRUE,
               showLegend=FALSE) + scale_color_manual(values=c("score"="darkviolet")) +
  geom_point(data=d, aes(x,y), size=3.5) + 
  labs(x="FDR (SNP-based)", y="TPR (SNP-based)")

p <- plot_grid( dblTypesScheme(), 
                p2 + theme(legend.position="none"), 
                scale=0.95, labels="AUTO")

# pdf("Figure4.pdf", width=9, height=4)
# p
# dev.off()
p
```

Adjusting for both types of 'misannotation' (i.e. homotypic doublet and missed within-individual doublets), the area under the PR curve is considerably better (0.82 instead of 0.64), and at the automatic threshold we estimate that `r round(d$y/(1-prop.homotypic)*100)`\% of heterotypic doublets can be identified with a real FDR of `r round(100*d$x-prop.intraind)`\% (a similar analysis for a different sample is shown in `r suppFig()`).

## Flexible thresholding for doublet calling

Most doublet detection methods provide a 'doublet score' that is higher on average in doublets than in singlets, 
and users are left to decide on a threshold above which droplets will be excluded as doublets.
Different methods have been suggested to this end.
Building on the fairly tight relationship (especially in 10x-based datasets) between the number of cells captured and the rate of doublets generated [@kangMultiplexedDropletSinglecell2018], some have set thresholds based on the number of doublets (or heterotypic doublets) one expects to find in the data [@mcginnisDoubletFinderDoubletDetection2019]. Others have used the best tradeoff in misclassifying artificial doublets from real droplets [@wolockScrubletComputationalIdentification2019].
Because `scDblFinder`'s scores come from a classifier, they are analogous to this tradeoff, and can directly be interpreted as a probability (not adjusting, however, for the base rate of doublets).

With true labels available, the benchmark datasets can again be used to evaluate thresholds. In most cases, we found the `scDblFinder` scores to change rapidly from high to low very close to the inflection point of the ROC curve (Figure \@ref(fig:thresholding)A), indicating that a fixed threshold (e.g. 0.5) can often be used.
In some cases, the scores are much more gradual, requiring a non-arbitrary way to set the thresholds.
`scDblFinder` therefore includes a thresholding method that combines both of the aforementioned rationales, and attempts to minimize both the proportion of artificial doublets being misclassified and the deviation from the expected doublet rate (see [Thresholding] and `r suppFig()`A).

Ideal thresholds defined by the ROC and PR curves, while not normally available in practice, can be used here to compare the different thresholding procedures.
The optimum represented by the elbow of the ROC curve gives equal weight to the _rate_ of both types of errors; however, due to the lower frequency of doublets, in absolute terms this amounts to considering a missed doublet worse that a wrongly excluded singlet.
Another ideal threshold can be defined from the PR curve, as the shortest distance to the corner defined by a perfect precision and recall.
This second optimum gives a more balanced weight to cells misclassified in one fashion or the other.
Figure \@ref(fig:thresholding)B compares the different thresholding procedure with respect to their deviation from both of these ideals.
The fixed score threshold and the scDblFinder combined threshold provide similar results, and are both clearly superior (with respect to both ideals) to thresholds based solely on the expected doublet rate.
Figure \@ref(fig:thresholding)C shows the TPR and FDR at each of the computed thresholds across datasets.


```{r thresholding, fig.height=7, fig.width=9, eval=TRUE, warning=FALSE, fig.cap="**Thresholding. A:** ROC curves (with square-root transformation on the x axis) of the different benchmark datasets, colored by scDblFinder doublet scores, showing a rapid flip of the scores around the inflexion point. The crosses indicate the scDblFinder thresholds. **B:** Deviation from two ideals of thresholds based on different methods. In the PR curve, the ideal is defined as the minimal distance from the corner indicating a perfect precision and recall. In the ROC curve, the ideal is defined as the maximal distance from the diagonal. The y-axis indicates the difference between the distance at the threshold and the respective optimal distance. **C:** Tradeoff between True Positive Rate (TPR/sensitivity/recall) and False Discovery Rate (FDR/1-precision) using different thresholds."}

ds <- readRDS("data/benchmark_datasets_called.CD2.rds")
ds <- lapply(ds, FUN=function(x){
  ndb <- nrow(x)*((nrow(x)*0.01)/1000)
  ndb <- round(ndb * (1-propHomotypic(x$scDblFinder.cluster)))
  th <- sort(x$scDblFinder.score, decreasing=TRUE)[ndb]
  x$called.dbrOnly <- factor(1+(x$scDblFinder.score>=th),1:2,c("singlet","doublet"))
  x
})
getRocs <- function(ds, score="scDblFinder.score", class="scDblFinder.class", merge=TRUE){
  ret <- lapply(ds, FUN=function(x){
    d <- data.frame(truth=as.integer(x$truth=="doublet"),
                    score=x[[score]],
                    called=x[[class]]=="doublet")
    d <- d[!is.na(d$truth),]
    d <- d[order(d$score, decreasing=TRUE),]
    d$FPR=cumsum(!d$truth)/sum(!d$truth)
    d$FDR=cumsum(!d$truth)/seq_along(d$truth)
    d$TPR=cumsum(d$truth)/sum(d$truth)
    d
  })
  if(merge) ret <- dplyr::bind_rows(ret, .id="Dataset")
  ret
}


plotROCscores <- function(rocs, legend=FALSE, addRandom=TRUE, rasterize=FALSE, fdr=FALSE){
  w <- which(!rocs$called & !duplicated(rocs[,c("Dataset","called")]))
  rot <- rocs[w-1,]
  if(fdr){
    p <- ggplot(rocs, aes(FDR, TPR, colour=score))
  }else{
    p <- ggplot(rocs, aes(FPR, TPR, colour=score)) + scale_x_sqrt()
  }
  p <- p + geom_path(colour="grey", aes(group=Dataset))
  if(rasterize){
    p <- p + ggrastr::geom_point_rast(size=0.8)
  }else{
    p <- p + geom_point(size=0.8)
  }
  p <- p + geom_point(data=rot, size=1, colour="black") + 
    geom_point(data=rot, shape=9, size=5) +
    scale_colour_viridis_c(direction=-1) + 
    #guides(colour=guide_colourbar(title.position="top", barwidth=8, titlle.hjust=0.5)) + 
    labs(colour="scDblFinder\nscore")
  if(legend){
    p <- p + theme(legend.position="bottom")
  }else{
    p <- p + theme(legend.position="none")
  }
  if(!fdr && addRandom) p <- p +
    geom_line(data=data.frame(FPR=(0:100)/100, TPR=(0:100)/100), 
              colour="darkgrey", linetype="dashed")
  p
}
rocs <- getRocs(ds)
w <- which(!rocs$called & !duplicated(rocs[,c("Dataset","called")]))
rot <- rocs[w-1,]
cols <- pipeComp::getQualitativePalette(length(ds))
names(cols) <- names(ds)

# obtain the deviation from the elbow
dr <- lapply(split(rocs, rocs$Dataset), FUN=function(x){
  x <- x[order(-x$score),]
  # signed distance to the diagonal
  x$proj <- rowMeans(x[,c("TPR","FPR")])
  dist2proj <- as.matrix(x[,c("TPR","FPR")])-x$proj
  dist2proj[,2] <- -dist2proj[,2]
  x$ddist <- sign(rowSums(dist2proj))*sqrt(rowSums(dist2proj^2))
  farthestPoint <- which.max(x$ddist)
  farthestDist <- x$ddist[farthestPoint]
  s50 <- head(which(x$score<=0.5),1)
  expected <- round(8*nrow(x)^2/10^6)
  clusters <- ds[[as.character(x$Dataset[[1]])]]$scDblFinder.cluster
  expected2 <- expected * (1-propHomotypic(clusters))
  th <- which(!x$called)[1]
  ll <- lapply(c("score 50%"=s50, "# expected"=expected, 
                 "# expected\nheterotypic"=expected2, "combined\nthreshold"=th),
               FUN=function(i){
    c(FPR=x$FPR[farthestPoint], TPR=x$TPR[farthestPoint], thFPR=x$FPR[i], thTPR=x$TPR[i], 
      distDev=x$ddist[i]-farthestDist,
      nbDev=i-farthestPoint,
      prop.dev=(i-farthestPoint)/nrow(x))
  })
  d <- as.data.frame(dplyr::bind_rows(ll, .id="method"))
})
dr <- dplyr::bind_rows(dr, .id="Dataset")
elbows <- dr[which(dr$method==dr$method[1]),]



prDR <- lapply(split(rocs, rocs$Dataset), FUN=function(x){
  x <- x[order(-x$score),]
  x$ddist <- sqrt((1-x$TPR)^2+(x$FDR)^2)
  closestPoint <- which.min(x$ddist)
  closestDist <- x$ddist[closestPoint]
  s50 <- head(which(x$score<=0.5),1)
  expected <- round(8*nrow(x)^2/10^6)
  clusters <- ds[[as.character(x$Dataset[[1]])]]$scDblFinder.cluster
  expected2 <- expected * (1-propHomotypic(clusters))
  th <- which(!x$called)[1]
  ll <- lapply(c("score 50%"=s50, "# expected"=expected,
                 "# expected\nheterotypic"=expected2, "combined\nthreshold"=th),
               FUN=function(i){
    c(FDR=x$FDR[closestPoint], TPR=x$TPR[closestPoint], thFDR=x$FDR[i], thTPR=x$TPR[i],
      distDev=x$ddist[i]-closestDist,
      nbDev=i-closestPoint,
      prop.dev=(i-closestPoint)/nrow(x))
  })
  d <- as.data.frame(dplyr::bind_rows(ll, .id="method"))
})
prDR <- dplyr::bind_rows(prDR, .id="Dataset")
dr2 <- rbind(cbind(type="ROC",dr[,c("method","distDev")]),
             cbind(type="PR",prDR[,c("method","distDev")]))


# p1 <- ggplot(rocs, aes(FPR, TPR, colour=Dataset)) + geom_line(size=1.3) + 
#   geom_point(data=elbows, size=4, colour="black") + geom_point(data=elbows, size=3) +
#   scale_x_sqrt() + scale_colour_manual(values=cols) +
#   guides(colour=guide_legend(title.position="top", title.hjust=0.5, ncol=3)) + 
#   theme(legend.position="bottom") + ggtitle("Distance to diagonal")

p1 <- plotROCscores(rocs, TRUE, rasterize=TRUE) + 
  theme(legend.position=c(1,0.25), legend.key.height=unit(0.5,"cm"), legend.justification = "right") +
  ggtitle("scDblFinder combined thresholds")
# p2 <- plotROCscores(rocs, TRUE, rasterize=TRUE, fdr=TRUE) + 
#   theme(legend.position="none")




p3 <- ggplot(dr2, aes(method, (distDev), fill=method)) + 
  geom_hline(yintercept=0, linetype="dashed") + geom_violin(, show.legend=FALSE) + 
  geom_boxplot(width=0.1, show.legend=FALSE) + facet_wrap(~type, scales = "free_y", nrow=2) + 
  labs(x="Thresholding", y="Difference to best distance") +
  theme(legend.key.height=unit(2, 'lines')) +
  ggtitle("Deviation from optimal thresholds")

ths <- dplyr::bind_rows(lapply(ds, FUN=function(x){
  x <- x[order(-x$scDblFinder.score),]
  expected <- round(8*nrow(x)^2/10^6)
  expected2 <- expected * (1-propHomotypic(x$scDblFinder.cluster))
  ths <- list("# expected"=expected, "# expected heterotypic"=expected2,
              "score 50%"=sum(x$scDblFinder.score>0.5),
              "combined"=sum(x$scDblFinder.class=="doublet"))
  x <- t(sapply(ths, FUN=function(th){
    y <- list(TP=sum(head(x$truth=="doublet", th)),
      FP=sum(head(x$truth!="doublet", th)),
      FN=sum(x$truth[-(1:th)]=="doublet"),
      TN=sum(x$truth[-(1:th)]!="doublet")
    )
    c(unlist(y), TPR=y$TP/(y$TP+y$FN), FDR=y$FP/(y$TP+y$FP), FPR=y$FP/(y$FP+y$TN))
  }))
  data.frame(method=row.names(x), x)
}), .id="dataset")
tpr <- reshape2::dcast(ths, method~dataset, value.var = "TPR")
fdr <- reshape2::dcast(ths, method~dataset, value.var = "FDR")
row.names(tpr) <- tpr[,1]
row.names(fdr) <- fdr[,1]
aucs <- data.frame(AUPRC=sort(sapply(split(rocs, rocs$Dataset), FUN=function(x){
    DescTools::AUC(x$TPR,1-x$FDR, from = 0, to=1)
  })))
fdr <- as.matrix(fdr[,row.names(aucs)])
tpr <- as.matrix(tpr[,row.names(aucs)])

ha <- function(label=TRUE){
  HeatmapAnnotation(df=aucs, show_annotation_name=label, show_legend = FALSE,
                    col=list(AUPRC=circlize::colorRamp2(seq(from=0,to=1,length.out=101),viridis::viridis(101))))
}
colsTPR <- circlize::colorRamp2(seq(from=0,to=1,length.out=101),viridis::inferno(101))
colsFDR <- circlize::colorRamp2(seq(from=0,to=1,length.out=101),viridis::mako(101,direction=-1))
p4 <- Heatmap(tpr, col=colsTPR, name="TPR", column_title = "TPR", row_title="Thresholding",
        cluster_columns = FALSE, bottom_annotation=ha(FALSE), column_names_gp=grid::gpar(fontsize=9),
        row_names_gp=grid::gpar(fontsize=11)) + 
    Heatmap(fdr, col=colsFDR, name="FDR", column_title = "FDR",
            cluster_columns = FALSE, bottom_annotation=ha(), column_names_gp=grid::gpar(fontsize=9),
            row_names_gp=grid::gpar(fontsize=11))
p4 <- grid.grabExpr(draw(p4, merge_legends=FALSE, 
                         align_heatmap_legend="global_center"))

p <- plot_grid(plot_grid(p1, p3, nrow=1, labels="AUTO", scale=0.9), p4,
               nrow=2, labels=c(NA,"C"), rel_heights=c(3,2), scale=c(1,0.9))

# pdf("Figure5.pdf", width=9, height=7)
# p
# dev.off()
p
```

## Doublet detection across multiple samples/captures

Multiple samples are often profiled and analyzed together, with the very common risk of batch effects (either technical or biological) across samples [@lutgeCellMixSQuantifyingVisualizing2021].
Therefore, while the droplets from all samples might in principle provide more information for doublet detection than a single sample can afford on its own, this must be weighted against the risk of bias due to batch differences.
To investigate this, we implemented different multi-sample approaches and tested them on two real multi-sample datasets with demuxlet-based true doublets, as well as a sub-sampling of them (Figure \@ref(fig:multisample)).

```{r multisample, fig.width=8, fig.height=4, fig.cap="**Comparison of four multi-sample strategies.** B1 and B2 the two batches from dataset GSE96583, and contain 3 and 2 captures, respectively. The datasets with the suffix 's' are versions downsampled to 30%. Using doublet detection on each capture separately (full split) was generally comparable to treating the captures as one (and adjusting the doublet rate)."}
res2 <- readRDS("../analyses/multisample_results.rds")
res2 <- reshape2::melt(res2, id.vars=c("method","dataset"))
res2 <- res2[res2$variable %in% c("AUPRC","AUROC"),]
res2$method <- as.factor(res2$method)
levels(res2$method) <- c("as one","single model\nsplit thresholds", "full split", "split with\nglobal clusters")
p <- ggplot(res2, aes(dataset, value, fill=method)) +
  geom_col(position="dodge") + facet_wrap(~variable) +
  labs(x="Dataset", y="") + coord_flip() + 
  theme(legend.position="bottom")

# pdf("Figure6.pdf", width=8, height=4)
# p
# dev.off()
p
```

The different multi-sample strategies had only a minor impact on the accuracy of the identification.
Based on these results, one could take the best overall strategy to be to process all samples as if they were one, however in our experience this can lead to biases against some samples when there are very large variations (e.g. in number of cells or coverage) across samples (not shown).
This approach also greatly increases running time.
In contrast, running the samples fully separately is computationally highly efficient, and is often equally accurate.
This being said, more multi-sample datasets with ground truth will be needed to establish the optimal procedure.

## Feature aggregation enables the use of scDblFinder on scATACseq

We next investigated whether `scDblFinder` could be applied to other types of single-cell data prone to doublets, such as single-cell ATAC sequencing (scATACseq). We compared scDblFinder to two methods specifically designed to scATACseq: the <a href="https://www.archrproject.com/bookdown/">`ArchR`</a> package [@granjaArchRScalableSoftware2021], which implements a doublet detection method that is also based on the comparison to artificial doublets, and the <a href="https://github.com/UcarLab/AMULET">`Amulet`</a> method [@thibodeauAMULETNovelRead2021]. `Amulet` is based on the assumption that, in a diploid cell, any given genomic region should be captured at most twice, and therefore interprets a larger number of loci with more than two reads as indicative of the droplet being a doublet. Since it was only available in the form of a mixture of java and python scripts, we re-implemented the method in the `scDblFinder` package, leading to highly comparable results (Figure \@ref(fig:scATAC)). Of note, the `Amulet` method has the advantage of capturing homotypic doublets, which tend to be missed by other methods.

The methods were compared across three datasets where a genotype-based annotation was available as ground truth: two obtained from @granjaArchRScalableSoftware2021 (<a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM4957261">GSM4957261</a> and <a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM4957262">GSM4957262</a>), which by design do not have homotypic doublets, and the dataset published along `Amulet` (<a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM5457171">GSM5457171</a>; @thibodeauAMULETNovelRead2021). The latter contains homotypic doublets, but its doublet annotation is highly incomplete: due to the low number of individuals multiplexed, we expected to have approximately 35\% of the doublets within-individual, and hence mislabeled as singlets.

With default parameters, `scDblFinder` performed very poorly (Figure \@ref(fig:scATAC)).
This is chiefly because `scDblFinder` follows the common scRNAseq strategy of selecting an informative subset of the features, while ATACseq reads are typically sparsely distributed across the genome. 
However, working with all features (i.e. peaks) is computationally very expensive.
An alternative to both approaches is to begin by reducing the size of the dataset by _aggregating_ correlated features into a relatively small set, thereby using information from all.
These aggregated features can then directly be used as the space in which to calculate distances.
This method yielded comparable performance to specialized single-cell ATACseq software (Figure \@ref(fig:scATAC)).

```{r scATAC, fig.height=3.5, fig.width=7, fig.cap="**Doublet identification in three single-nucleus ATAC-seq datasets.** 'amulet.py' and 'amulet.R' respectively stand for the original and R reimplementation of the method. 'scDblFinder.agg' stands for the feature aggregation approach. 'combination' indicates a Fisher combination of the amulet.R p-value and the 1 minus the scDblFinder.agg score. For 'ArchR', the DoubletEnrichment output was used."}
ov <- readRDS("../other_datasets/atac.results.rds")
ov$combined.p <- 1-apply(1-cbind(ov$amulet.R,ov$scDblFinder.agg), 1, FUN=function(x){
  x[x<0.001] <- 0.001 # prevent too much skew from very small or 0 p-values
  suppressWarnings(aggregation::fisher(x))
})

ld <- split(ov, ov$dataset)
stats <- lapply(setNames(names(ld),names(ld)), FUN=function(x){
  x <- ld[[x]]
  scores <- intersect(c("amulet.py", "amulet.R", "scDblFinder.raw","scDblFinder.agg", "DoubletEnrichment", "clamulet", "combined.p"), colnames(x))
  x <- x[!is.na(x$truth),]
  if(any(hasNA <- rowSums(is.na(as.matrix(x[,scores])))>0)){
    warning(sum(hasNA), " cells discarded because of NA values!")
    x <- x[!hasNA,]
  }
  dplyr::bind_rows(lapply(setNames(scores,scores), FUN=function(y){
    s <- split(x[[y]], x$truth)
    c(AUPRC=mean(as.numeric(PRROC::pr.curve(s[[2]], s[[1]])[2:3])),
      AUROC=PRROC::roc.curve(s[[2]], s[[1]])[[2]])
  }), .id="method")
})
stats <- dplyr::bind_rows(stats, .id="Dataset")
stats$Dataset <- factor(stats$Dataset, names(ld))
s2 <- rbind(stats,stats)
s2$method <- factor(s2$method, rev(c("DoubletEnrichment","amulet.py","amulet.R","clamulet","scDblFinder.raw", "scDblFinder.agg", "combined.p")))
levels(s2$method)[length(levels(s2$method))] <- "ArchR"
s2$metric <- rep(c("AUROC","AUPRC"), each=nrow(stats))
s2$value <- c(stats$AUROC, stats$AUPRC)
p <- ggplot(s2, aes(method, value, fill=metric)) + geom_col(position=position_dodge()) + facet_wrap(~Dataset) + coord_flip()

# pdf("Figure7.pdf", width=7, height=3.5)
# p
# dev.off()
p
```

While of an elegant simplicity, the `Amulet` approach performed well only on one dataset (Figure \@ref(fig:scATAC)). However, the authors indicate that larger library sizes are needed for the approach to perform well, which is not the case for most cells in these datasets. Another problem is that the number of loci with more than two reads is strongly dependent on library size (`r suppFig()`), however this dependency cannot easily be taken into account because ATAC doublets also tend to have a larger library size, making the two variables confounded.

Since none of the methods appeared clearly superior across all datasets, we next investigated two ways of combining the logics of `scDblFinder` (aggregation) and Amulet. First, we developed the `clamulet` method (for classifier-powered Amulet), which mimics the `scDblFinder` workflow but creates artificial doublets from coverages, enabling the use, as part of the predictions, the number of loci covered by more than two reads. We also tried running both methods separately and aggregating the resulting *p*-values using Fisher's method (which proved better than averages or rank-based aggregation). This proved to be the most satisfactory approach, providing result that are more robust across datasets (Figure \@ref(fig:scATAC). This being said, more complex ways of aggregating calls from different methods could be explored [@xiongChordIdentifyingDoublets2021; @neavinDemuxafyImprovementDroplet2022], and more work, and especially on a broader set of benchmark datasets, will be necessary to establish optimal methods.

## Doublet origins and enrichment analysis

When artificial doublets are generated between clusters, we know which clusters constitute them. We therefore reasoned that this information could be used to infer the clusters composing *real* doublets (hereafter referred to as 'doublet origin'). Using a simulation as well as the aforementioned real dataset with doublets of known origins (mixture of five cell lines from @tianScRNAseqMixologyBetter2018), we first assessed the accuracy of doublet origin prediction based on the nearest artificial doublets in the kNN. These proved inaccurate, both in real and simulated data (see `r suppFig()`A-B). Even training a classifier directly on this problem failed (see `r suppFig(FALSE)`C-D).
The problem appears to be that, due to the very large variations in library sizes (and related variations in relative contributions of the composing cells -- see Figure \@ref(fig:realDbls)B), doublets often contain a large fraction of reads from one cell type, and conversely a small fraction from the other cell type.
As a consequence, we can typically call at least one of the two originating cell types, but seldom both.
In the real dataset, at least one of the two originating cell types is correctly identified in 73\% of doublets (random expectation: 36\%), but both are correct in only 20% of cases.

While the identification of doublet origins remains a challenge, for the sake of completeness we nevertheless developed strategies to investigate whether certain doublet types were found more often than expected.
Such enrichment could, for instance, indicate cell-to-cell interactions.
We defined two forms of doublet enrichment (Figure \@ref(fig:dblenr)A-B), and specified models to test each possibility:
i) enrichment in doublets formed by a specific combination of celltypes, or
ii) enrichment in doublets involving a given cell type, denoted `sticky'.


```{r dblenrScheme}
cln <- c(a=500,b=100,c=300,d=500)
n <- probs <- (cln/sum(cln)) %*% t(cln/sum(cln))
row.names(probs) <- row.names(n) <- names(cln)
probs[lower.tri(n)] <- n[lower.tri(n)] <- NA
diag(probs) <- diag(n) <- NA
probs <- probs/sum(probs,na.rm=TRUE)
set.seed(123)
probs1 <- probs2 <- probs
probs1[,4] <- probs1[,4]*2.5
probs2[2,4] <- probs2[2,4]*2.5
n1 <- n2 <- n
n1[upper.tri(n)] <- rpois(6,lambda=1500*probs1[upper.tri(n)])
n2[upper.tri(n)] <- rpois(6,lambda=1500*probs2[upper.tri(n)])
#n[lower.tri(n)] = t(n)[lower.tri(n)]

enr1 <- ( (n1/sum(n1,na.rm=TRUE))/probs )
enr2 <- ( (n2/sum(n2,na.rm=TRUE))/probs )

pdh <- function(x, column_title="Cell types", column_title_side="top", ...,
                col=circlize::colorRamp2(c(0,1,2.5), c("blue","lightgrey","red"))){
  Heatmap(x[-4,-1], col=col, cluster_rows=FALSE, cluster_columns=FALSE, na_col="white", 
          column_names_side="top", column_names_rot=90, row_title="Cell types", 
          column_title=column_title, column_title_side=column_title_side,
          width=unit(2.5,"cm"), height=unit(2.5,"cm"), ...)
}
```

```{r, fig.height=2.5, fig.width=8.5}
p1 <- plot_grid(
  plot_grid(
    ggplot(data.frame(celltype=names(cln), proportion=cln/sum(cln)), aes(celltype, proportion)) + 
      geom_col() + theme_cowplot() + xlab("Cell types") + ggtitle("  "),
    grid.grabExpr(draw(pdh(probs, name="% of\ndbls", col=viridis::viridis(100)))),
    scale=c(0.8,1)
  ),
  NULL,
  grid.grabExpr(draw((pdh(enr1, column_title="'Sticky'\ncell type", show_heatmap_legend=FALSE) + 
                       pdh(enr2, name="fold-\nenrichment", column_title="Enriched\ncombination")), 
                     column_title="Cell types", column_title_side="bottom")),
  labels=c(" A: Expected doublet proportions", ""," B: Enrichment scenarios"),
  nrow=1, hjust=0, rel_widths=c(6,1,5)
)
```


The `stickiness' of each cluster (as proxy for cell types) can be evaluated by fitting a single generalized linear model on the observed abundance of doublets of each origin (see [Methods]).
We tested the performance of this test under different underlying distributions using simulated doublet counts.
The number of doublets of each type is generated from random expectation with or without added stickiness (as factors of 1 to 3 on the probability) using negative binomial distributions with different over-dispersion parameters (Figure \@ref(fig:dblenr)C and `r suppFig()`).
The quasi-binomial showed the best performance, followed by the negative binomial, but in all cases the p-values were not well calibrated and many false positives were reported at a nominal FDR<0.05.
This was robust across different over-dispersion values (see `r suppFig(FALSE)`).


```{r}
load("../analyses/enrichment_results.RData")
scores <- stick.scores
cols <- setNames(scales::hue_pal()(length(scores)), names(scores))
p2 <- plotROCs(lapply(scores, FUN=function(x) 1-x$FDR), scores[[2]]$truth, th=0.95, size=4, fdr=TRUE) + scale_colour_manual(values=cols)
leg <- ggpubr::get_legend(p2)
scores <- comb.scores
p3 <- plotROCs(lapply(scores, FUN=function(x) 1-x$FDR), scores[[2]]$truth, th=0.95, size=4, fdr=TRUE) + theme(legend.position="none")
```

```{r dblenr, fig.width=8, fig.height=6, fig.cap="**Doublet enrichment analysis. A-B:** Doublet enrichment in a toy example. **A:** Proportion of different doublet types from random expectations based on the cell type abundances. **B:** The fold-enrichment over this expectation in two different doublet enrichment scenarios. **C-D:** Performance of the cluster stickiness tests (C) and tests for enrichment of specific combinations (D) using different underlying distributions."}
p <- plot_grid(p1,
  plot_grid(
    p2 + ggtitle("Cluster stickiness") + theme(legend.position="none"), 
    p3 + ggtitle("Specific combinations"), leg,
    nrow=1, rel_widths=c(3,3,1), scale=0.95, labels=c("C","D","")),
  nrow=2, rel_heights=c(2.1,3))

# pdf("Figure8.pdf", width=8, height=6)
# p
# dev.off()
p
```

We next sought to establish a test for the enrichment of specific combinations.
Here, we simply computed the probability of the observed counts for each combination using different models (see [Methods]).
We again tested this approach relying on different underlying distributions, on simulations with varying over-dispersion (Figure \@ref(fig:dblenr)C).
The negative binomial performed best, however all variants suffered a high false discovery rate.


# Conclusions

The `scDblFinder` package includes a set of efficient methods for doublet detection in both single-cell RNA and ATAC sequencing. In particular, the main `scDblFinder` approach integrates insights from previous approaches into a comprehensive doublet detection method that provides robustly accurate detection across a number of benchmark datasets, at a considerably greater speed and scalability than the best alternatives. Even in complex datasets, most heterotypic doublets can be accurately identified. Although the doublet scores given by `scDblFinder` can be directly interpreted as probabilities, simplifying their interpretation, the method also includes a trade-off thresholding procedure incorporating doublet rate expectations with classification optimization, thereby facilitating its usage.

`scDblFinder` additionally provides utilities for identifying the origins of doublets (in terms of composing cell types) and testing for different forms of doublet enrichment. At present, however, the value of such tests is limited by the difficulty of accurately identifying doublet origins. Further research will be needed to assess to what extent this can be improved.

In conclusion, we believe that `scDblFinder`, with its flexibility, accuracy and scalability, represents a key resource for doublet detection in high-throughput single-cell sequencing data.




# Methods

## scDblFinder implementation

As a first step, the dataset is reduced to its top most expressed features (1000 by default); if the cluster-based approach is used, the top features per cluster are instead selected.

The generation of artificial doublets then depends on whether the clustered or random mode is used.
If using the cluster-based approach (and not manually specifying the clusters), a fast clustering is performed (see [Fast clustering]).
Artificial doublets are then created by combining random droplets of different clusters, proportionally to the cluster sizes.
In explicitly concentrating on between-cluster doublets, we do not attempt to identify homotypic doublets, which are anyway virtually unidentifiable and relatively innocuous.
In doing so, we reduce the necessary number of artificial doublets (since no artificial doublet is 'lost' modeling homotypic doublets), and prevent the classifier from being trained to recognize doublets that are indistinguishable from singlets, which would lead to calling singlets as doublets.

An alternative strategy also available in `scDblFinder` is to generate fully random artificial doublets, and use the iterative procedure (see below) to exclude unidentifiable artificial doublets from the training.
In practice, the two approaches have comparable performances (Figure \@ref(fig:benchmark1)), and they can also be combined.

Dimension reduction is then performed on the union of real and artificial droplets, and a nearest neighbor network is generated.
The network is then used to estimate a number of characteristics for each cell, in particular the proportion of artificial doublets among the nearest neighbors.
Rather than selecting a specific neighborhood size, the ratio is calculated at different values of *k*, creating multiple predictors that will be used by the classifier.
A distance-weighted ratio is also included.
Further cell-level predictors are added, including: projections on principal components; library size; number of detected features; and co-expression scores [based on a variation of @baisScdsComputationalAnnotation2020].
`scDblFinder` then trains gradient boosted trees to distinguish, based on these features, artificial doublets from real cells.
Finally, a thresholding procedure decides the score at which to call a droplet by simultaneously minimizing the misclassification rate and the expected doublet rate (see [Thresholding]).

### Artificial doublet generation

For artificial doublet generation, only cells with library sizes with the 5-95 percentiles are used. 
Doublets are then created using random pairs of cells (or random between-cluster pairs, if clusters are used).
The majority (75\% by default) are generated by summing the counts of the two cells.
For the remaining the sum is divided by two and used as mean for poisson sampling to yield counts.
If clusters are used, for half of these last doublets the contributions of the two cells are also re-weighted using the clusters' median library size: rather than the two cells contributing to the doublet based on the ratio of their actual library size, this is averaged with the ratio of the median library size of their respective cluster.

### Parameter optimization

Using the benchmark datasets from @xiBenchmarkingComputationalDoubletDetection2021, we next optimized a number of parameters in the procedure, notably regarding features to include and hyperparameters, so as to provide robust default parameters (see `r suppFig(4)`).
Some features, such as the distance to the nearest doublet or whether the nearest neighbor is an artificial doublet, had a negative impact on performance (see `r suppFig(nb=-3)`), presumably because it led to over-fitting.
Finally, in line with a discrepancy between the trained and real problems, we observed that the variable importance calculated during training (see `r suppFig(nb=-2)`) did not necessarily match that of the variable drop experiments (see `r suppFig(nb=-3)`).

We finally optimized learning hyperparameters (see `r suppFig(nb=-1)`) and further input parameters (see `r suppFig(FALSE)`).

### Fast clustering

Irlba-based singular value decomposition is first run using the `r Biocpkg("BiocSingular")` package, and a kNN network is generated using the Annoy approximation implemented in `r Biocpkg("BiocNeighbors")`.
Louvain clustering is then used on the graph.
If the dataset is sufficiently large (>1000 cells), a first rapid *k*-means clustering is used to generate a large number of meta-cells, which are then clustered using the graph-based approach, propagating clusters back to the cells themselves.

### Thresholding

Unless manually given, the expected number of doublets ($e$) is specified by $e = n^2/10^-5$ (where $n$ is the number of cells captured).
This is then restricted to heterotypic doublets using random expectation from cluster sizes or, if not using the cluster-based approach, using the proportion of artificial doublets misidentified.
The doublet rate is accompanied by an uncertainty interval (`dbr.sd` parameter), and the deviation from the expected doublet number for threshold $t$ is then calculated as

$$
deviation_t = \begin{cases}
  0 & 
    \text{if } (o_t \geq e_{low} \land o_t \leq e_{high}) \\
  2 \cdot \frac{
  \min(|o_t-e_{low}|, |o_t-e_{high}|)
}{e_{low} + e_{high}} & \text{otherwise}
\end{cases}
$$

where $o_t$ represents the number of real droplets classified as doublets at threshold $t$, and $e_{low}$ and $e_{high}$ represent, respectively, the lower and higher bounds of the expected number of heterotypic doublets in the dataset (based on the given or estimated doublet rate $\pm$ the `dbr.sd` parameter).
The default value of the `dbr.sd` parameter was roughly estimated from the variability of observed doublet rates (`r suppFig(nb=4)`B). 
The cost function being minimized is then simply given by $cost_t = FNR_t + FPR_t + deviation_t^2$, where the false negative rate ($FNR_t$) represents the proportion of artificial doublets misclassified as singlets at threshold $t$, and the false positive rate ($FPR_t$) represents the proportion of real droplets classified as doublets. This is illustrated in `r suppFig(nb=4)`A.

Since this is performed in an iterative fashion, the $FPR$ is calculated ignoring droplets which were called as doublets in the previous round.

## Doublet enrichment analysis

### Cluster stickiness

Cluster `stickiness' can be evaluated by fitting a single generalized linear model on the observed abundance of doublets of each origin, in the following way:

$$
\log(observed_i+0.1) = \log(e_i) + \beta_z \cdot \log(difficulty_i) +
\beta_a a_i + \beta_b b_i + \beta_c c_i + ... +\epsilon_i ,
$$
where $observed_i$ and $e_i$ represent the numbers of doublets formed by specific combination $i$ of clusters which are respectively observed or expected from random combinations, and $a_i$, $b_i$ and $c_i$ (etc) indicate whether or not (0/1) the doublet involves each cluster.

Because some doublets are easier to identify than others, some deviation from their expected abundance is typically observed.
For this reason, a $\text{difficulty}_i$ term is optionally included, indicating the difficulty in identifying doublets of origin $i$, estimated from the misclassification of `scDblFinder`'s artificial doublets of that origin (by default, the term is included if at least 7 clusters are present).
A $\beta_a$ significantly different from zero, then, indicates that cluster _a_ forms more or less doublets than expected -- if positive, it indicates cluster `stickiness'.

For the (quasi-)binomial distributions, logit was used instead of log transformation, and the mean of observed and expected counts was used as observational weights.

### Enrichment for specific combinations

To account for the different identification difficulty across doublet types, we first fit the following global negative binomial model: 

$$
\log(observed_i) = \alpha + \log(expected_i) + \beta \cdot \log(difficulty_i),
$$
where $\text{observed}_i$ and $\text{expected}_i$ are respectively the observed and theoretically expected number of doublets of type $i$, and the $\text{difficulty}_i$ term is the same as for the stickiness problem above. 
Then, the fitted values are then considered the expected abundance, and a p-value for each doublet type is given by the probability of the observed count under this adjusted expected value, using either distribution (for the negative binomial, the global over-dispersion parameter calculated in the first step is used).

## Direct classification

The direct classification approach is implemented in the `directDblClassification` function of the package. It uses the same doublet generation, thresholding and iterative learning procedures as `scDblFinder`, but trains directly on the normalized expression matrix of real and artificial droplets instead of kNN-based features. The hyperparameters were the same except for the maximum tree depth, which was increased to 6 to account for the increased complexity of the predictors.

## Feature aggregation

For feature aggregation (used for scATACseq), `scDblFinder` first normalizes the counts using the Term Frequency - Inverse Document Frequency (TF-IDF) normalization, as implemented in @stuartComprehensiveIntegrationSingleCell2019. PCA is then performed and the features are clustered into the desired number of meta-features using mini-batch *k*-means [@hicksMbkmeansFastClustering2021] or, if not available, simple *k*-means. The counts are then summed per meta-feature.

## Benchmark

### Datasets

We used the scRNAseq benchmark datasets prepared by @xiBenchmarkingComputationalDoubletDetection2021, which were originally published by @kangMultiplexedDropletSinglecell2018, @stoeckiusCellHashingBarcoded2018, @mcginnisDoubletFinderDoubletDetection2019, @mcginnisMULTIseqSampleMultiplexing2019, and @wolockScrubletComputationalIdentification2019.

### Metrics

The area under the PR or ROC curves were calculated using integral method, implemented in the `PRROC` package [@grauPRROCComputingVisualizing]. The adjusted AUPRC, meant to capture the AUPRC accounting for homotypic and within-individual doublets, was calculated as the proportion of the unshaded area in Figure 3. Specifically, values were linearly scaled values so that an observed FDR of corresponding to the expected proportion of within-individual doublets is set to 0, and that an observed TPR corresponding to one minus the expected proportion of homotypic doublets as an adjusted TPR of 1. Values were capped to be within a 0-1 range, and the area under the curve was calculated using trapezoid approximation. The expected proportion of homotypic doublets was estimated using the clusters from the fast clustering method described above (see [Fast clustering]).

The reported metrics are an average of the results of two runs using different random seeds.

# scDblFinder operation

`scDblFinder` is provided as a <a href="https://www.bioconductor.org">bioconductor</a> package.
The input data for `scDblFinder` (denoted `x` below) can be either 
i) a count matrix (full or sparse), with genes/features as rows and cells/droplets as columns; or
ii) an object of class `r Biocpkg("SingleCellExperiment")`.
In either case, the object should not contain empty drops, but should not otherwise have undergone very stringent filtering (which would bias the estimate of the doublet rate). The doublet detection can then be launched with:

```{r, echo=TRUE, eval=FALSE}
library(scDblFinder)
x <- scDblFinder(x)
```

The output is a `r Biocpkg("SingleCellExperiment")` object including all of the input data, as well as a number of columns to the `colData` slot, the most important of which are:

* `sce$scDblFinder.score` : the final doublet score (the higher the more likely that the droplet is a doublet)
* `sce$scDblFinder.class` : the binary classification (doublet or singlet)

For more details, see the package's <a href="https://bioconductor.org/packages/devel/bioc/vignettes/scDblFinder/inst/doc/scDblFinder.html">vignettes</a>.


# Software availability

`scDblFinder` is available from Bioconductor: http://www.bioconductor.org/packages/release/bioc/html/scDblFinder.html

The source code is available from: https://github.com/plger/scDblFinder 

Archived source code at time of publication: https://doi.org/10.6084/m9.figshare.16543518.v1

The software is released under the GNU Public License (GPL-3).

# Extended data

https://doi.org/10.6084/m9.figshare.16617571.v1

This repository contains the following extended data:

* Supplementary Figures (see Extended Data - Figures 1-10)

Data are available under the terms of the Creative Commons BY 4.0 license.

# Underlying data

https://doi.org/10.6084/m9.figshare.16543518.v1

This repository contains the following underlying data:

* scDblFinder 1.7.4 (archived software version used in the paper)
* scDblFinder_paper (code to reproduce the analyses and figures) 

The code to reproduce the analyses and figures is additionally available at https://github.com/plger/scDblFinder_paper

Data are available under the terms of the Creative Commons BY 4.0 license.

# Author information

PLG developed the scDblFinder method and performed the analyses, and wrote the paper with feedback from all authors.
AL contributed alternatives methods to the package and provided support.
CGM investigated improvements to the Amulet strategy.
WM provided general feedback and testing, and helped with the design of the enrichment tests.
MDR supervised and funded the project.

# Competing interests

No competing interests were disclosed.

# Grant information

This work was supported by the Swiss National Science Foundation (grant number 310030_175841). MDR acknowledges support from the University Research Priority Program Evolution in Action at the University of Zurich.

# Acknowledgments

We thank Nan Miles Xi and Jingyi Jessica Li for help with their benchmark datasets; Jeffrey Granja for support with ArchR and its attached datasets; and the Robinson group, users and reviewers for feedback.


# References
